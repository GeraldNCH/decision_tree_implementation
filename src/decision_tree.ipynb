{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eNq4lCSSQWn"
      },
      "source": [
        "**Instituto Tecnológico de Costa Rica - TEC**\n",
        "\n",
        "***Inteligencia Artificial***\n",
        "\n",
        "*Docente: Kenneth Obando Rodríguez*\n",
        "\n",
        "---\n",
        "# Trabajo Corto 4: Árboles de Decisión\n",
        "---\n",
        "Estudiantes:\n",
        "- Gerald Núñez Chavarría - 2021023226\n",
        "- Sebastián Arroniz Rojas - 2021108521\n",
        "- Sebastián Bermúdez Acuña - 2021110666\n",
        "\n",
        "Link del Cuaderno (recuerde configurar el acceso a público):\n",
        "\n",
        "    \n",
        "- [Repositorio de GitHub](https://github.com/GeraldNCH/decision_tree_implementation)\n",
        "\n",
        "    **Nota:** Este trabajo tiene como objetivo promover la comprensión de la materia y su importancia en la elección de algoritmos. Los alumnos deben evitar copiar y pegar directamente información de fuentes externas, y en su lugar, demostrar su propio análisis y comprensión.\n",
        "\n",
        "### Entrega\n",
        "Debe entregar un archivo comprimido por el TecDigital, incluyendo un documento pdf con los resultados de los experimentos y pruebas. La fecha de entrega es el jueves 2 de mayo, antes de las 10:00pm.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmAGdF_-Xhj1"
      },
      "source": [
        "Instrucciones:\n",
        "\n",
        "Las alternativas se rifarán en clase utilizando números aleatorios. Deberá realizar la asignación propuesta. Si realiza ambos ejercicios, recibirá 20 puntos en **la nota porcentual de la actividad**, para aplicar a la totalidad de los puntos extra es necesario que ambas actividades se completen al 100%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onCmI27E3Jit"
      },
      "source": [
        "## Actividad - Taller\n",
        "\n",
        "1. Cree una clase nodo con atributos necesarios para un árbol de decisión: feature, umbral, gini, cantidad_muestras, valor, izquierda, derecha\n",
        "\n",
        "2. Crea una clase que implementa un árbol de decisión, utilice las funciones presentadas en clase, además incluya los siguientes hyperparámetros:\n",
        "   - max_depth: Cantidad máxima de variables que se pueden explorar\n",
        "   - min_split_samples: Cantidad mínima de muestras que deberá tener un nodo para poder ser dividido\n",
        "   - criterio: función que se utilizará para calcular la impuridad.\n",
        "\n",
        "3. Divida los datos en los conjuntos tradicionales de entrenamiento y prueba, de forma manual, sin utilizar las utilidades de sklearn (puede utilizar índices de Numpy o Pandas)\n",
        "\n",
        "4. Implemente una función que se llame `validacion_cruzada` que entrene $k$ modelos y reporte las métricas obtenidasd:\n",
        "  a. Divida el conjunto de entrenamiento en $k$ subconjuntos excluyentes\n",
        "  b. Para cada uno de los $k$ modelos, utilice un subconjunto como validación\n",
        "  c. Reporte la media y la desviación estándar para cada una de las métricas, todo debe realizarse solo usando Numpy:\n",
        "    - Accuracy\n",
        "    - Precision\n",
        "    - Recall\n",
        "    - F1\n",
        "  \n",
        "5. Entrene 10 combinaciones distintas de parámetros para su implementación de Arbol de Decisión y utilizando su implementación de `validacion_cruzada`.\n",
        "\n",
        "6. Utilizando los resultados obtenidos analice cuál y porqué es el mejor modelo para ser usado en producción.\n",
        "\n",
        "7. Compruebe las métricas usando el conjunto de prueba y analice el resultado\n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementación\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtener la data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_radius</th>\n",
              "      <th>mean_texture</th>\n",
              "      <th>mean_perimeter</th>\n",
              "      <th>mean_area</th>\n",
              "      <th>mean_smoothness</th>\n",
              "      <th>diagnosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
              "0        17.99         10.38          122.80     1001.0          0.11840   \n",
              "1        20.57         17.77          132.90     1326.0          0.08474   \n",
              "2        19.69         21.25          130.00     1203.0          0.10960   \n",
              "3        11.42         20.38           77.58      386.1          0.14250   \n",
              "4        20.29         14.34          135.10     1297.0          0.10030   \n",
              "\n",
              "   diagnosis  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  "
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"../docs/data/Breast_cancer_data.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Cree una clase nodo con atributos necesarios para un árbol de decisión: feature, umbral, gini, cantidad_muestras, valor, izquierda, derecha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DTNode:\n",
        "    def __init__(self, feature=None, threshold=None, gini=None, sample_count=None, left=None, right=None, value=None):\n",
        "        self.feature = feature  # Used for splitting\n",
        "        self.threshold = threshold  # Threshold for division\n",
        "        self.gini = gini\n",
        "        self.sample_count = sample_count  # Number of samples in this node\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "\n",
        "        # for leaf\n",
        "        self.value = value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Crea una clase que implementa un árbol de decisión, utilice las funciones presentadas en clase, además incluya los siguientes hiperparámetros:\n",
        "   - max_depth: Cantidad máxima de variables que se pueden explorar\n",
        "   - min_split_samples: Cantidad mínima de muestras que deberá tener un nodo para poder ser dividido\n",
        "   - criterio: función que se utilizará para calcular la impuridad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, max_depth, min_split_samples, criterion='gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_split_samples = min_split_samples \n",
        "        self.criterion = criterion # gini or entropy\n",
        "\n",
        "        self.root = None  # Root of tree\n",
        "\n",
        "    \"\"\"Recursive function to build the tree\"\"\"\n",
        "    def buildTree(self, dataset, currDepth=0):\n",
        "        X, Y = dataset[:, :-1], dataset[:, -1] # Splitting the features and target variable\n",
        "        numSamples, numFeatures = np.shape(X)\n",
        "\n",
        "        # Split until stopping conditions are met\n",
        "        if numSamples >= self.min_split_samples and currDepth <= self.max_depth:\n",
        "            bestSplit = self.getBestSplit(dataset, numFeatures)\n",
        "\n",
        "            if bestSplit[self.criterion] > 0: # To make sure to not split a pure node\n",
        "\n",
        "                # Left subtree\n",
        "                leftSubtree = self.buildTree(bestSplit['dataset_left'], currDepth + 1)\n",
        "\n",
        "                # Right subtree\n",
        "                rightSubtree = self.buildTree(bestSplit['dataset_right'], currDepth + 1)\n",
        "\n",
        "                # Return node\n",
        "                return DTNode(bestSplit['feature'],\n",
        "                              bestSplit['threshold'],\n",
        "                              bestSplit[self.criterion],\n",
        "                              numSamples,\n",
        "                              leftSubtree,\n",
        "                              rightSubtree)\n",
        "            \n",
        "        leafValue = self.calculateLeafValue(Y) # If stopping conditions are met, return a leaf node\n",
        "        return DTNode(value=leafValue)\n",
        "    \n",
        "    \"\"\"Find the best split from all the features\"\"\"\n",
        "    def getBestSplit(self, dataset, numFeatures):\n",
        "        bestSplit = {} # To store the best split info\n",
        "        maxInfoGain = -float('inf')\n",
        "\n",
        "        # Loop through all features\n",
        "        for featureIdx in range(numFeatures):\n",
        "            featureValues = dataset[:, featureIdx]\n",
        "            possibleThresholds = np.unique(featureValues)\n",
        "\n",
        "            # Loop through all unique values of the feature\n",
        "            for threshold in possibleThresholds:\n",
        "                datasetLeft, datasetRight = self.split(dataset, featureIdx, threshold) # Split the dataset\n",
        "\n",
        "                if len(datasetLeft) > 0 and len(datasetRight) > 0: # Make sure it is not empty\n",
        "                    y, leftY, rightY = dataset[:, -1], datasetLeft[:, -1], datasetRight[:, -1] # Extracting targets\n",
        "                    infoGain = self.calculateInfoGain(y, leftY, rightY)\n",
        "\n",
        "                    # Update the best split\n",
        "                    if infoGain > maxInfoGain:\n",
        "                        bestSplit['feature'] = featureIdx\n",
        "                        bestSplit['threshold'] = threshold\n",
        "                        bestSplit[self.criterion] = infoGain\n",
        "                        bestSplit['dataset_left'] = datasetLeft\n",
        "                        bestSplit['dataset_right'] = datasetRight\n",
        "\n",
        "                        maxInfoGain = infoGain\n",
        "\n",
        "        return bestSplit\n",
        "    \n",
        "    \"\"\"Split the dataset into left and right subtrees\"\"\"\n",
        "    def split(self, dataset, featureIdx, threshold):\n",
        "        datasetLeft = np.array([row for row in dataset if row[featureIdx] <= threshold]) # Left subtree, less than or equal to threshold\n",
        "        datasetRight = np.array([row for row in dataset if row[featureIdx] > threshold]) # Right subtree, greater than threshold\n",
        "        return datasetLeft, datasetRight\n",
        "    \n",
        "    \"\"\"Calculate the info_gain value, corresponding to the worth of a trio of parent and its possible childs\"\"\"\n",
        "    def calculateInfoGain(self, parent, lChild, rChild):\n",
        "        # Importance (weight) of each child in respect to their parent\n",
        "        weight_l = len(lChild) / len(parent) \n",
        "        weight_r = len(rChild) / len(parent)\n",
        "\n",
        "        if self.criterion == 'gini':\n",
        "            return self.calculateGini(parent) - (weight_l * self.calculateGini(lChild) + weight_r * self.calculateGini(rChild))\n",
        "        else:\n",
        "            return self.calculateCriterion(parent) - (weight_l * self.calculateCriterion(lChild) + weight_r * self.calculateCriterion(rChild))\n",
        "    \n",
        "    \"\"\"Calculate the gini impurity\"\"\"\n",
        "    def calculateGini(self, y):\n",
        "        classes = np.unique(y)\n",
        "        gini = 0\n",
        "        for cls in classes:\n",
        "            p = len(y[y == cls]) / len(y)\n",
        "            gini += p * (1 - p)\n",
        "        return 1 - gini\n",
        "    \n",
        "    def calculateCriterion(self, y):\n",
        "        # Not Implemented\n",
        "        pass\n",
        "    \n",
        "    \"\"\"Calculate the most common value in the leaf\"\"\"\n",
        "    def calculateLeafValue(self, y):\n",
        "        Y = list(y)\n",
        "        return max(Y, key=Y.count) # Return the most common value in the leaf\n",
        "    \n",
        "    \"\"\"Train the tree\"\"\"\n",
        "    def fit(self, X, Y):\n",
        "        Y = np.expand_dims(Y, axis=1)\n",
        "        dataset = np.concatenate((X, Y), axis=1)\n",
        "        self.root = self.buildTree(dataset)\n",
        "    \n",
        "    \"\"\"Predict a new data set\"\"\"\n",
        "    def predict(self, X):\n",
        "        preditions = [self.makePrediction(x, self.root) for x in X]\n",
        "        return preditions\n",
        "    def makePrediction(self, x, tree):\n",
        "        if tree.value!=None: return tree.value\n",
        "        feature_val = x[tree.feature_index]\n",
        "        if feature_val<=tree.threshold:\n",
        "            return self.makePrediction(x, tree.left)\n",
        "        else:\n",
        "            return self.makePrediction(x, tree.right)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Divida los datos en los conjuntos tradicionales de entrenamiento y prueba, de forma manual, sin utilizar las utilidades de sklearn (puede utilizar índices de Numpy o Pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_radius</th>\n",
              "      <th>mean_texture</th>\n",
              "      <th>mean_perimeter</th>\n",
              "      <th>mean_area</th>\n",
              "      <th>mean_smoothness</th>\n",
              "      <th>diagnosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
              "0        17.99         10.38          122.80     1001.0          0.11840   \n",
              "1        20.57         17.77          132.90     1326.0          0.08474   \n",
              "2        19.69         21.25          130.00     1203.0          0.10960   \n",
              "3        11.42         20.38           77.58      386.1          0.14250   \n",
              "4        20.29         14.34          135.10     1297.0          0.10030   \n",
              "\n",
              "   diagnosis  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  "
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainRatio = 0.6  # Ratio to divide data.\n",
        "trainSize = int(trainRatio * len(data))\n",
        "\n",
        "trainData = data.iloc[:trainSize]  # Set for training\n",
        "testData = data.iloc[trainSize:]  # Set for testing\n",
        "\n",
        "# Separate features (X) and labels (Y) in each set\n",
        "X_train = trainData.drop(columns=['diagnosis'])\n",
        "Y_train = trainData['diagnosis']\n",
        "\n",
        "X_test = testData.drop(columns=['diagnosis'])\n",
        "Y_test = testData['diagnosis']\n",
        "\n",
        "trainData.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Implemente una función que se llame `validacion_cruzada` que entrene $k$ modelos y reporte las métricas obtenidasd:\n",
        "  a. Divida el conjunto de entrenamiento en $k$ subconjuntos excluyentes\n",
        "  b. Para cada uno de los $k$ modelos, utilice un subconjunto como validación\n",
        "  c. Reporte la media y la desviación estándar para cada una de las métricas, todo debe realizarse solo usando Numpy:\n",
        "    - Accuracy\n",
        "    - Precision\n",
        "    - Recall\n",
        "    - F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crossValidation(model, X_train, Y_train, k=5):\n",
        "    foldSize = len(X_train) // k  # Size of each fold\n",
        "    metrics = {'accuracy': [], 'precision': [], 'recall': []}  # To store the metrics for each fold\n",
        "\n",
        "    for i in range(0, len(X_train), foldSize):\n",
        "        # Test and Train set for this fold\n",
        "        X_valuesFold = X_train[i:i + foldSize]\n",
        "        Y_valuesFold = Y_train[i:i + foldSize]\n",
        "        X_trainFold = np.concatenate((X_train[:i], X_train[i + foldSize:])) \n",
        "        Y_trainFold = np.concatenate((Y_train[:i], Y_train[i + foldSize:]))\n",
        "\n",
        "        model.fit(X_trainFold, Y_trainFold)  # Train the model\n",
        "        predictions = model.predict(X_valuesFold)  # Predict the test set\n",
        "\n",
        "        # Calculate the metrics\n",
        "        accuracy = np.mean(predictions == Y_valuesFold)\n",
        "        precision = np.sum((predictions == 1) & (Y_valuesFold == 1)) / np.sum(predictions == 1)\n",
        "        recall = np.sum((predictions == 1) & (Y_valuesFold == 1)) / np.sum(Y_valuesFold == 1)\n",
        "\n",
        "        # Store metrics\n",
        "        metrics['accuracy'].append(accuracy)\n",
        "        metrics['precision'].append(precision)\n",
        "        metrics['recall'].append(recall)\n",
        "\n",
        "    meanMetrics = {metric: np.mean(values) for metric, values in metrics.items()}\n",
        "    stdMetrics = {metric: np.std(values) for metric, values in metrics.items()}\n",
        "\n",
        "    return meanMetrics, stdMetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Entrene 10 combinaciones distintas de parámetros para su implementación de Arbol de Decisión y utilizando su implementación de `validacion_cruzada`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model 1 with params: {'max_depth': 3, 'min_split_samples': 2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/tz/06nn_cjs5nxc2rkr6mvzdxx00000gn/T/ipykernel_30188/3008803668.py:17: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  precision = np.sum((predictions == 1) & (Y_valuesFold == 1)) / np.sum(predictions == 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for model 1:\n",
            "accuracy: 0.5368 (±0.2475)\n",
            "precision: nan (±nan)\n",
            "recall: 0.0000 (±0.0000)\n",
            "--------------------\n",
            "Training model 2 with params: {'max_depth': 5, 'min_split_samples': 2}\n",
            "Metrics for model 2:\n",
            "accuracy: 0.5368 (±0.2475)\n",
            "precision: nan (±nan)\n",
            "recall: 0.0000 (±0.0000)\n",
            "--------------------\n",
            "Training model 3 with params: {'max_depth': 2, 'min_split_samples': 5}\n",
            "Metrics for model 3:\n",
            "accuracy: 0.5368 (±0.2475)\n",
            "precision: nan (±nan)\n",
            "recall: 0.0000 (±0.0000)\n",
            "--------------------\n",
            "Training model 4 with params: {'max_depth': 3, 'min_split_samples': 3}\n",
            "Metrics for model 4:\n",
            "accuracy: 0.5368 (±0.2475)\n",
            "precision: nan (±nan)\n",
            "recall: 0.0000 (±0.0000)\n",
            "--------------------\n",
            "Training model 5 with params: {'max_depth': 2, 'min_split_samples': 2}\n",
            "Metrics for model 5:\n",
            "accuracy: 0.5368 (±0.2475)\n",
            "precision: nan (±nan)\n",
            "recall: 0.0000 (±0.0000)\n",
            "--------------------\n",
            "Training model 6 with params: {'max_depth': 5, 'min_split_samples': 3}\n",
            "Metrics for model 6:\n",
            "accuracy: 0.5368 (±0.2475)\n",
            "precision: nan (±nan)\n",
            "recall: 0.0000 (±0.0000)\n",
            "--------------------\n",
            "Training model 7 with params: {'max_depth': 3, 'min_split_samples': 5}\n",
            "Metrics for model 7:\n",
            "accuracy: 0.5368 (±0.2475)\n",
            "precision: nan (±nan)\n",
            "recall: 0.0000 (±0.0000)\n",
            "--------------------\n",
            "Training model 8 with params: {'max_depth': 5, 'min_split_samples': 5}\n",
            "Metrics for model 8:\n",
            "accuracy: 0.5368 (±0.2475)\n",
            "precision: nan (±nan)\n",
            "recall: 0.0000 (±0.0000)\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "def trainModel(X_train, Y_train):\n",
        "    params = [\n",
        "        {'max_depth': 3, 'min_split_samples': 2},\n",
        "        {'max_depth': 5, 'min_split_samples': 2},\n",
        "        {'max_depth': 2, 'min_split_samples': 5},\n",
        "        {'max_depth': 3, 'min_split_samples': 3},\n",
        "        {'max_depth': 2, 'min_split_samples': 2},\n",
        "        {'max_depth': 5, 'min_split_samples': 3},\n",
        "        {'max_depth': 3, 'min_split_samples': 5},\n",
        "        {'max_depth': 5, 'min_split_samples': 5},\n",
        "    ]\n",
        "\n",
        "    for i, params in enumerate(params):\n",
        "        print(f\"Training model {i + 1} with params: {params}\")\n",
        "        \n",
        "        tree = DecisionTree(params['max_depth'], params['min_split_samples'])\n",
        "        meanMetrics, stdMetrics = crossValidation(tree, X_train, Y_train, k=5) # Get cross validation metrics\n",
        "        \n",
        "        # Imprimir las métricas obtenidas\n",
        "        print(f\"Metrics for model {i + 1}:\")\n",
        "        for metric, value in meanMetrics.items():\n",
        "            print(f\"{metric}: {value:.4f} (±{stdMetrics[metric]:.4f})\")\n",
        "        print(\"-\"*20)\n",
        "\n",
        "trainModel(X_train.to_numpy(), Y_train.to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Utilizando los resultados obtenidos analice cuál y porqué es el mejor modelo para ser usado en producción."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Compruebe las métricas usando el conjunto de prueba y analice el resultado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_9m6Jqr25fi"
      },
      "source": [
        "## Rúbrica para la Implementación de un Árbol de Decisión\n",
        "\n",
        "**Nota: Esta rúbrica se basa en la calidad de la implementación y los resultados obtenidos, no en la cantidad de código.**\n",
        "\n",
        "**1. Creación de la Clase Nodo (10 puntos)**\n",
        "\n",
        "- [ ] Se crea una clase `Nodo` con los atributos mencionados en las especificaciones (feature, umbral, gini, cantidad_muestras, valor, izquierda, derecha).\n",
        "- [ ] Los atributos se definen correctamente y se asignan de manera apropiada.\n",
        "\n",
        "**2. Creación de la Clase Árbol de Decisión (20 puntos)**\n",
        "\n",
        "- [ ] Se crea una clase que implementa un árbol de decisión.\n",
        "- [ ] La clase utiliza las funciones presentadas en el cuaderno.\n",
        "- [ ] Se implementan los hyperparámetros solicitados (max_depth, min_split_samples, criterio).\n",
        "- [ ] La clase es capaz de entrenar un árbol de decisión con los hyperparámetros especificados.\n",
        "\n",
        "**3. División de Datos (10 puntos)**\n",
        "\n",
        "- [ ] Los datos se dividen en conjuntos de entrenamiento y prueba de forma manual.\n",
        "- [ ] Se utiliza Numpy o Pandas para realizar esta división.\n",
        "- [ ] Se garantiza que los conjuntos sean excluyentes.\n",
        "\n",
        "**4. Implementación de Validación Cruzada (20 puntos)**\n",
        "\n",
        "- [ ] Se implementa la función `validacion_cruzada` correctamente.\n",
        "- [ ] Los datos de entrenamiento se dividen en k subconjuntos excluyentes.\n",
        "- [ ] Se entrena y evalúa un modelo para cada subconjunto de validación.\n",
        "- [ ] Se calculan y reportan las métricas de accuracy, precision, recall y F1.\n",
        "- [ ] Se calcula la media y la desviación estándar de estas métricas.\n",
        "\n",
        "**5. Entrenamiento de Modelos (20 puntos)**\n",
        "\n",
        "- [ ] Se entrenan 10 combinaciones distintas de parámetros para el árbol de decisión.\n",
        "- [ ] Cada combinación se entrena utilizando la función `validacion_cruzada`.\n",
        "- [ ] Los resultados de las métricas se registran adecuadamente.\n",
        "\n",
        "**6. Análisis de Modelos (10 puntos)**\n",
        "\n",
        "- [ ] Se analizan los resultados obtenidos y se selecciona el mejor modelo para ser utilizado en producción.\n",
        "- [ ] Se proporciona una justificación clara y fundamentada sobre por qué se eligió ese modelo.\n",
        "\n",
        "**7. Prueba en el Conjunto de Prueba (10 puntos)**\n",
        "\n",
        "- [ ] Se comprueban las métricas del modelo seleccionado utilizando el conjunto de prueba.\n",
        "- [ ] Se analizan los resultados y se comentan las conclusiones.\n",
        "\n",
        "**General (10 puntos)**\n",
        "\n",
        "- [ ] El código se documenta de manera adecuada, incluyendo comentarios que expliquen las secciones clave.\n",
        "- [ ] El código se ejecuta sin errores y sigue buenas prácticas de programación.\n",
        "- [ ] La presentación de los resultados es clara y fácil de entender.\n",
        "- [ ] Se cumple con todos los requisitos y las especificaciones proporcionadas.\n",
        "\n",
        "**Puntuación Total: 100 puntos**\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
